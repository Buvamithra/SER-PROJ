# ğŸ¤ Speech Emotion Recognition (SER) using Deep Learning

This project is a **Speech Emotion Recognition (SER)** system that detects human emotions from speech audio using a Deep Neural Network. It classifies emotions such as **Angry**, **Happy**, **Sad**, **Fearful**, **Calm**, **Disgust**, and **Neutral** by analyzing audio features (MFCCs) and can even respond to high-risk emotional states in real-time.

> ğŸ¯ Ideal for applications in mental health monitoring, customer service analytics, and interactive AI agents.

---

## ğŸ“Œ Features

- ğŸ™ï¸ Real-time emotion recognition from live microphone input
- ğŸ“ Trained on **RAVDESS** â€” a well-known emotional speech dataset
- ğŸ“Š MFCC feature extraction and visualization
- ğŸ§  DNN-based model achieving high accuracy
- ğŸ”” Alerts when high-risk emotions (like *Angry*, *Fear*, or *Anxiety*) are detected with >98% confidence
- ğŸ’» Simple GUI for interaction

---

## ğŸ§  Emotion Classes

The model predicts the following emotions:

- ğŸ˜  Angry  
- ğŸ˜¢ Sad  
- ğŸ˜¨ Fear  
- ğŸ˜„ Happy  
- ğŸ˜ Neutral  
- ğŸ˜Œ Calm  
- ğŸ¤¢ Disgust

---

## ğŸ“‚ Folder Structure

SER-Project/
â”œâ”€â”€ dataset/ # RAVDESS dataset (download externally)
â”œâ”€â”€ extracted_features/ # CSV files containing MFCC features
â”œâ”€â”€ mfcc_plots/ # MFCC plots for visual inspection
â”œâ”€â”€ models/ # Saved trained model (.h5)
â”œâ”€â”€ real_time_test.py # Real-time microphone testing
â”œâ”€â”€ train_model.py # Training script for the model
â”œâ”€â”€ feature_extraction.py # Extract MFCCs from dataset
â”œâ”€â”€ final_gui.py # GUI interface for the system
â”œâ”€â”€ alarm.mp3 # Alert for high-risk emotions
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

yaml
Copy
Edit

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/speech-emotion-recognition.git
cd speech-emotion-recognition
2. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Download the RAVDESS Dataset
Download it from Zenodo and place it inside the dataset/ folder.

âš™ï¸ Run the Project
â¤ Extract MFCC Features
bash
Copy
Edit
python feature_extraction.py
This will save features in extracted_features/ and MFCC plots in mfcc_plots/.

â¤ Train the Model
bash
Copy
Edit
python train_model.py
Trained model will be saved in the models/ directory.

â¤ Real-time Emotion Recognition
bash
Copy
Edit
python real_time_test.py
This will record 10 seconds from your mic, detect emotion, and play an alert if needed.

â¤ GUI Interface (Optional)
bash
Copy
Edit
python final_gui.py
Includes emotion prediction display, background visuals, and alert sounds.

ğŸ”Š High-Risk Emotion Alert
If an emotion such as Angry, Fear, or Anxiety is detected with more than 98% confidence, the system plays an alert sound (alarm.mp3). This feature is useful for:

ğŸš¨ Emergency response systems

ğŸ§“ Elderly/child emotion tracking

ğŸ§  Mental wellness monitoring

ğŸ“ˆ Model Details
Algorithm: Deep Neural Network (Keras, TensorFlow backend)

Input Features: 40 MFCCs per sample

Dataset: RAVDESS (Audio-only subset)

Validation Accuracy: ~95%+

Loss Function: Categorical Crossentropy

Optimizer: Adam

ğŸ“Š Visual Output
MFCC plots for each audio file

Accuracy/Loss curve plots

Confusion matrix for final evaluation

ğŸ–¼ï¸ Sample Screenshot
(Add a screenshot here if you'd like. You can use a GUI screenshot or MFCC plots.)

ğŸ§ª Requirements
All dependencies are listed in requirements.txt. Install them using:

bash
Copy
Edit
pip install -r requirements.txt
Main libraries used:

librosa

tensorflow / keras

numpy, pandas

sounddevice

matplotlib

scikit-learn

tkinter (for GUI)

ğŸ“¬ Author
Buvamithra Ulagarajan
ğŸ“ Final Year B.E (ECE), Aspiring AI Engineer
ğŸ“§ buvamithraulagarajan@gmail.com
ğŸ”— LinkedIn Profile

ğŸ“„ License
This project is open-source and available under the MIT License. Feel free to modify, distribute, and use for academic or commercial purposes.
